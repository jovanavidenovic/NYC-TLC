{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b1c0fe",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T7: Forecast Uber waiting time</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from datetime import timedelta\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.ensemble import BlockwiseVotingRegressor\n",
    "import dask.array as da\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import dask as dxgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.preprocessing import Categorizer\n",
    "from dask_ml.wrappers import Incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f1a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(errors, n_iterations=1000):\n",
    "    \"\"\"Perform bootstrapping to estimate confidence intervals.\"\"\"\n",
    "    n_size = len(errors)\n",
    "    indices = np.random.randint(0, n_size, (n_iterations, n_size))\n",
    "    samples = errors[indices]\n",
    "    means = np.mean(samples, axis=1)\n",
    "    lower = np.percentile(means, 2.5)\n",
    "    upper = np.percentile(means, 97.5)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2, threads_per_worker=2, memory_limit='32GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3e76dc",
   "metadata": {},
   "source": [
    "#### Loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/d/hpc/projects/FRI/bigdata/students/jv8043/partitioned_data/fhvhv/2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54183195",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(data_path, engine=\"pyarrow\", assume_missing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50354c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c82448",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[ddf[\"hvfhs_license_num\"] == \"HV0003\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1464cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_columns = [\n",
    "    \"request_datetime\",\n",
    "    \"on_scene_datetime\",\n",
    "    \"pulocationid\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20178e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf[important_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eccf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"request_datetime\"] = dd.to_datetime(ddf[\"request_datetime\"])\n",
    "ddf[\"on_scene_datetime\"] = dd.to_datetime(ddf[\"on_scene_datetime\"])\n",
    "ddf[\"pulocationid\"] = ddf[\"pulocationid\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e90a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan\n",
    "ddf = ddf.dropna(subset=[\"request_datetime\", \"on_scene_datetime\", \"pulocationid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320d94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['wait_time'] = (ddf['on_scene_datetime'] - ddf['request_datetime']).dt.total_seconds() / 60\n",
    "ddf['wait_time'] = ddf['wait_time'].astype('int64')\n",
    "ddf = ddf[ddf['wait_time'] >= 0]\n",
    "ddf = ddf[ddf['wait_time'] <= 30]  # Assuming wait time is capped at 30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddf['on_scene_datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704ddbd",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c49a1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(dt):\n",
    "    month = dt.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'autumn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(train_df, test_df, categorical_cols):\n",
    "    train_df = train_df.categorize(columns=categorical_cols)\n",
    "    print(categorical_cols, test_df.columns)\n",
    "    test_df = test_df.categorize(columns=categorical_cols)\n",
    "\n",
    "    # One-hot encode using Dask\n",
    "    train_encoded = dd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "    test_encoded = dd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Align columns in test to match train\n",
    "    missing_cols = [col for col in train_encoded.columns if col not in test_encoded.columns]\n",
    "    for col in missing_cols:\n",
    "        test_encoded[col] = 0\n",
    "\n",
    "    extra_cols = [col for col in test_encoded.columns if col not in train_encoded.columns]\n",
    "    test_encoded = test_encoded.drop(columns=extra_cols)\n",
    "\n",
    "    # Ensure column order matches\n",
    "    test_encoded = test_encoded[train_encoded.columns]\n",
    "\n",
    "    return train_encoded, test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf['request_dayofweek'] = ddf['request_datetime'].dt.dayofweek\n",
    "# create sin and cos features for hour of day\n",
    "ddf['request_hour'] = ddf['request_datetime'].dt.hour\n",
    "ddf['request_minute'] = ddf['request_datetime'].dt.minute\n",
    "\n",
    "ddf['request_sin_hour'] = np.sin(2 * np.pi * ddf['request_hour'] / 24)\n",
    "ddf['request_cos_hour'] = np.cos(2 * np.pi * ddf['request_hour'] / 24)\n",
    "\n",
    "ddf['request_sin_minute'] = np.sin(2 * np.pi * ddf['request_minute'] / 60)\n",
    "ddf['request_cos_minute'] = np.cos(2 * np.pi * ddf['request_minute'] / 60)\n",
    "\n",
    "ddf['request_day_of_week_sin'] = np.sin(2 * np.pi * ddf['request_dayofweek'] / 7)\n",
    "ddf['request_day_of_week_cos'] = np.cos(2 * np.pi * ddf['request_dayofweek'] / 7)\n",
    "\n",
    "# categorical features\n",
    "ddf['request_season'] = ddf['request_datetime'].apply(get_season)\n",
    "ddf[\"weekend\"] = ddf[\"request_dayofweek\"].isin([5, 6]).astype(int)\n",
    "\n",
    "del ddf['request_dayofweek']\n",
    "del ddf['request_datetime']\n",
    "del ddf['request_hour']\n",
    "del ddf['request_minute']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f2996",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ac59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply 1he to pulocationid\n",
    "\n",
    "features = ['pulocationid', 'request_sin_hour', 'request_cos_hour',\n",
    "       'request_sin_minute', 'request_cos_minute', 'request_day_of_week_sin',\n",
    "       'request_day_of_week_cos', 'request_season', 'weekend']\n",
    "X = ddf[features]\n",
    "y = ddf['wait_time']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ece6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "# ignore indexs, just have matrices / arrays\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c11aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode categorical features\n",
    "categorical_cols = ['pulocationid', 'request_season']\n",
    "X_train, X_test = one_hot_encode(X_train, X_test, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.persist()\n",
    "for col in X_test.columns:\n",
    "    try:\n",
    "        X_test[col] = X_test[col].persist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error persisting column {col}: {e}\")\n",
    "y_train = y_train.persist()\n",
    "y_test = y_test.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdacd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use incremental training for large data\n",
    "model = Incremental(estimator=SGDRegressor())\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbfacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3cad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde6939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = model.predict(X_test).compute()\n",
    "\n",
    "# Evaluate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test.compute(), y_pred)\n",
    "print(f'MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e46482",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.compute().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
