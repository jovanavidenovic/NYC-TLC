{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b1c0fe",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T7: Forecasting demand for services separately</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from datetime import timedelta\n",
    "from dask_ml.linear_model import LinearRegression\n",
    "from dask_ml.ensemble import BlockwiseVotingRegressor\n",
    "import dask.array as da\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "from xgboost import dask as dxgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e25b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping(errors, n_iterations=1000):\n",
    "    \"\"\"Perform bootstrapping to estimate confidence intervals.\"\"\"\n",
    "    n_size = len(errors)\n",
    "    indices = np.random.randint(0, n_size, (n_iterations, n_size))\n",
    "    samples = errors[indices]\n",
    "    means = np.mean(samples, axis=1)\n",
    "    lower = np.percentile(means, 2.5)\n",
    "    upper = np.percentile(means, 97.5)\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=2, threads_per_worker=2, memory_limit='64GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fc7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/d/hpc/home/jv8043/BD/project/T7/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01611407",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_y = pd.read_csv(data_path / \"dc_y_augmented.csv\")\n",
    "dc_g = pd.read_csv(data_path / \"dc_g_augmented.csv\")\n",
    "dc_fhv = pd.read_csv(data_path / \"dc_fhv_augmented.csv\")\n",
    "dc_fhvhv = pd.read_csv(data_path / \"dc_fhvhv_augmented.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only from 2019-03-01 to 2019-03-31\n",
    "dc_y = dc_y[(dc_y[\"pickup_datetime\"] >= \"2019-03-01\") & (dc_y[\"pickup_datetime\"] < \"2025-01-15\")]\n",
    "dc_g = dc_g[(dc_g[\"pickup_datetime\"] >= \"2019-03-01\") & (dc_g[\"pickup_datetime\"] < \"2025-01-15\")]\n",
    "dc_fhv = dc_fhv[(dc_fhv[\"pickup_datetime\"] >= \"2019-03-01\") & (dc_fhv[\"pickup_datetime\"] < \"2025-01-15\")]\n",
    "dc_fhvhv = dc_fhvhv[(dc_fhvhv[\"pickup_datetime\"] >= \"2019-03-01\") & (dc_fhvhv[\"pickup_datetime\"] < \"2025-01-15\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e5fdec",
   "metadata": {},
   "source": [
    "### Minimal setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed617a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(train_df, test_df, categorical_cols):\n",
    "    train_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "    test_encoded = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "    test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)\n",
    "    return train_encoded, test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00633a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b002b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.zeros((test_size, 1))\n",
    "final_gt = np.zeros((test_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efad4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for daily_counts, s_name in zip([dc_y, dc_g, dc_fhv, dc_fhvhv], [\"dc_y\", \"dc_g\", \"dc_fhv\", \"dc_fhvhv\"]):   \n",
    "    # add season column\n",
    "    def get_season(dt):\n",
    "        month = dt.month\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'summer'\n",
    "        else:\n",
    "            return 'autumn'\n",
    "\n",
    "    daily_counts['pickup_datetime'] = pd.to_datetime(daily_counts['pickup_datetime'])\n",
    "    daily_counts['season'] = daily_counts['pickup_datetime'].apply(get_season)\n",
    "    daily_counts['day'] = daily_counts['pickup_datetime'].dt.day\n",
    "    daily_counts['week_in_month'] = (daily_counts['pickup_datetime'].dt.day - 1) // 7 + 1\n",
    "    daily_counts['day_of_year'] = daily_counts['pickup_datetime'].dt.dayofyear\n",
    "    daily_counts['day_of_week'] = daily_counts['pickup_datetime'].dt.dayofweek\n",
    "    daily_counts['year'] = daily_counts['pickup_datetime'].dt.year\n",
    "    daily_counts['month'] = daily_counts['pickup_datetime'].dt.month\n",
    "    daily_counts[\"weekend\"] = daily_counts[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    if \"GB\" not in MODEL_NAME:\n",
    "        daily_counts[\"day_of_week_sin\"] = np.sin(2 * np.pi * daily_counts[\"day_of_week\"] / 7)\n",
    "        daily_counts[\"day_of_week_cos\"] = np.cos(2 * np.pi * daily_counts[\"day_of_week\"] / 7)\n",
    "        daily_counts[\"month_sin\"] = np.sin(2 * np.pi * daily_counts[\"month\"] / 12)\n",
    "        daily_counts[\"month_cos\"] = np.cos(2 * np.pi * daily_counts[\"month\"] / 12)\n",
    "        daily_counts[\"day_of_year_sin\"] = np.sin(2 * np.pi * daily_counts[\"day_of_year\"] / 365)\n",
    "        daily_counts[\"day_of_year_cos\"] = np.cos(2 * np.pi * daily_counts[\"day_of_year\"] / 365)\n",
    "\n",
    "        del daily_counts[\"month\"]\n",
    "        del daily_counts['year']  \n",
    "    else:\n",
    "        pass\n",
    "        # from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "        # # === 1. Define custom categorization functions for each feature ===\n",
    "\n",
    "        # def categorize_awnd(val):  # Average wind speed\n",
    "        #     if val < 4:\n",
    "        #         return 'windy'\n",
    "        #     else:\n",
    "        #         return 'very_windy'\n",
    "\n",
    "        # def categorize_prcp(val):  # Precipitation\n",
    "        #     if val == 0:\n",
    "        #         return 'none'\n",
    "        #     else:\n",
    "        #         return 'rain'\n",
    "           \n",
    "        # def categorize_snow(val):  # Snowfall\n",
    "        #     if val == 0:\n",
    "        #         return 'none'\n",
    "        #     else:\n",
    "        #         return 'snow'\n",
    "            \n",
    "        # def categorize_snwd(val):  # Snow depth\n",
    "        #     if val == 0:\n",
    "        #         return 'none'\n",
    "        #     else:\n",
    "        #         return 'snow'\n",
    "\n",
    "        # def categorize_temp(temp):  # For TMAX and TMIN\n",
    "        #     if temp < 0:\n",
    "        #         return 'freezing'\n",
    "        #     elif temp < 15:\n",
    "        #         return 'cold'\n",
    "        #     else:\n",
    "        #         return 'hot'\n",
    "\n",
    "        # # === 2. Apply categorizations ===\n",
    "\n",
    "        # daily_counts['AWND_cat'] = daily_counts['AWND'].apply(categorize_awnd)\n",
    "        # daily_counts['PRCP_cat'] = daily_counts['PRCP'].apply(categorize_prcp)\n",
    "        # daily_counts['SNOW_cat'] = daily_counts['SNOW'].apply(categorize_snow)\n",
    "        # daily_counts['SNWD_cat'] = daily_counts['SNWD'].apply(categorize_snwd)\n",
    "        # daily_counts['TMAX_cat'] = daily_counts['TMAX'].apply(categorize_temp)\n",
    "        # daily_counts['TMIN_cat'] = daily_counts['TMIN'].apply(categorize_temp)\n",
    "\n",
    "        # # === 3. Encode categories with LabelEncoder ===\n",
    "\n",
    "        # categorical_cols = ['AWND_cat', 'PRCP_cat', 'SNOW_cat', 'SNWD_cat', 'TMAX_cat', 'TMIN_cat']\n",
    "        \n",
    "        # for col in categorical_cols:\n",
    "        #     le = LabelEncoder()\n",
    "        #     encoded_col = col + '_enc'\n",
    "        #     daily_counts[encoded_col] = le.fit_transform(daily_counts[col])\n",
    "        \n",
    "        # del daily_counts['AWND']\n",
    "        # del daily_counts['PRCP']\n",
    "        # del daily_counts['SNOW']\n",
    "        # del daily_counts['SNWD']\n",
    "        # del daily_counts['TMAX']\n",
    "        # del daily_counts['TMIN']\n",
    "        # del daily_counts['AWND_cat']\n",
    "        # del daily_counts['PRCP_cat']\n",
    "        # del daily_counts['SNOW_cat']\n",
    "        # del daily_counts['SNWD_cat']\n",
    "        # del daily_counts['TMAX_cat']\n",
    "        # del daily_counts['TMIN_cat']\n",
    "\n",
    "    print(daily_counts.columns)\n",
    "    daily_counts = daily_counts[daily_counts['ride_count'] > 1000]\n",
    "    del daily_counts[\"pickup_datetime\"]\n",
    "    del daily_counts[\"day_of_year\"]\n",
    "    del daily_counts[\"day_of_week\"]\n",
    "\n",
    "    train = daily_counts.iloc[:-test_size]\n",
    "    test = daily_counts.iloc[-test_size:]\n",
    "    if \"GB\" not in MODEL_NAME:\n",
    "        train, test = one_hot_encode(train, test, ['season'])\n",
    "    else:\n",
    "        train, test = one_hot_encode(train, test, ['season', 'month', 'year'])\n",
    "        \n",
    "    cols = train.columns.difference(['ride_count', 'pickup_datetime'])\n",
    "\n",
    "    for c in cols:\n",
    "        train[c] = train[c].astype('float')\n",
    "        test[c] = test[c].astype('float')\n",
    "\n",
    "    # Convert to Dask arrays\n",
    "    # select all but 'ride_count' and 'pickup_datetime'\n",
    "    X_train = da.from_array(train[cols].values)\n",
    "    y_train = da.from_array(train['ride_count'].values)\n",
    "\n",
    "    X_test = da.from_array(test[cols].values)\n",
    "    y_test = da.from_array(test['ride_count'].values)\n",
    "    \n",
    "    final_gt += y_test.compute().reshape(-1, 1)\n",
    "\n",
    "\n",
    "    if MODEL_NAME == \"DXGB\":\n",
    "        dtrain = dxgb.DaskDMatrix(client, X_train, y_train)\n",
    "        output = dxgb.train(\n",
    "            client,\n",
    "            {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:linear\"},\n",
    "            dtrain,\n",
    "            num_boost_round=4,\n",
    "            evals=[(dtrain, \"train\")],\n",
    "    )    \n",
    "        prediction = dxgb.predict(client, output, X_test)\n",
    "        y_pred = prediction.compute()\n",
    "    elif MODEL_NAME == \"XGB\":\n",
    "        est = dxgb.XGBRegressor()\n",
    "        est.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        # Predict\n",
    "        y_pred = est.predict(X_test)\n",
    "    elif MODEL_NAME == \"LGB\":\n",
    "        dask_model = lgb.DaskLGBMRegressor(client=client)\n",
    "        dask_model = dask_model.fit(X_train, y_train)\n",
    "        y_pred = dask_model.predict(X_test).compute()\n",
    "    else:\n",
    "        est = BlockwiseVotingRegressor(\n",
    "            estimator= LinearRegression(),\n",
    "        )\n",
    "        # Fit the model\n",
    "        est.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred = est.predict(X_test).compute()\n",
    "\n",
    "    final_pred += y_pred.reshape(-1, 1)\n",
    "\n",
    "    # Calculate and print metrics\n",
    "    absolute_errors = np.abs(y_test.compute() - y_pred)\n",
    "    lower, upper = bootstrapping(absolute_errors)\n",
    "    \n",
    "    print(f\"Confidence intervals for {s_name}:\")\n",
    "    print(f\"Lower bound: {lower}\")\n",
    "    print(f\"Upper bound: {upper}\")\n",
    "    print(\"MAE\", np.mean(absolute_errors))\n",
    "\n",
    "    mape_errors = np.abs((y_test.compute() - y_pred) / y_test.compute())\n",
    "    mape_lower, mape_upper = bootstrapping(mape_errors)\n",
    "    \n",
    "    print(f\"Confidence intervals for MAPE {s_name}:\")\n",
    "    print(f\"Lower bound: {mape_lower}\")\n",
    "    print(f\"Upper bound: {mape_upper}\")\n",
    "    print(\"MAPE\", np.mean(mape_errors))\n",
    "\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06563efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_errors = np.abs(final_gt - final_pred)\n",
    "lower, upper = bootstrapping(absolute_errors)\n",
    "\n",
    "print(\"Final confidence intervals:\")\n",
    "print(f\"Lower bound: {lower}\")\n",
    "print(f\"Upper bound: {upper}\")\n",
    "print(\"Final MAE\", np.mean(absolute_errors))\n",
    "\n",
    "print(\"-\" * 40)\n",
    "mape_errors = np.abs((final_gt - final_pred) / final_gt)\n",
    "mape_lower, mape_upper = bootstrapping(mape_errors)\n",
    "\n",
    "print(\"Final confidence intervals for MAPE:\")\n",
    "print(f\"Lower bound: {mape_lower}\")\n",
    "print(f\"Upper bound: {mape_upper}\")\n",
    "print(\"Final MAPE\", np.mean(mape_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mape\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "mape = mean_absolute_percentage_error(final_gt, final_pred) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
