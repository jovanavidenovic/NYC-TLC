{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b1c0fe",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T2: Invalid data points analysis</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "47c5a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f6f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_data_path = Path(\"/d/hpc/projects/FRI/bigdata/students/jv8043/partitioned_data\")\n",
    "service_type = \"fhvhv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a68f56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_path = Path(\"/d/hpc/home/jv8043/BD/project/T2/T2_tables\") / service_type\n",
    "os.makedirs(tables_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8a0fd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1, memory_limit='64GB')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1786f7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecb5f26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_outliers_fhvhv(df, start_date, end_date):\n",
    "    # map: parallel file / partition processing: compares datetime columns independently for each partition in parallel\n",
    "    # reduce: group & aggregate : aggregates results across all partitions\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "\n",
    "    # check if dropoff is equal to pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] == df['pickup_datetime']] # map\n",
    "    d_eq_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff is before pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] < df['pickup_datetime']] # map\n",
    "    d_before_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff year is equal to the pickup year\n",
    "    # tolerate if the difference is 1 day (New Year's Eve to New Year's Day)\n",
    "    invalid_rows = df[(df['dropoff_datetime'].dt.year != df['pickup_datetime'].dt.year) & \n",
    "                      (df['dropoff_datetime'] - df['pickup_datetime'] > pd.Timedelta(days=1))] # map\n",
    "    d_year_diff_res = invalid_rows.groupby('year').size().compute() # reduce \n",
    "\n",
    "    # check how many trips are before start_date\n",
    "    invalid_rows = df[df['pickup_datetime'] < start_date] # map\n",
    "    d_start_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['pickup_datetime'] > end_date] # map\n",
    "    d_pu_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['dropoff_datetime'] > end_date] # map\n",
    "    d_do_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # number of trips longer than 24 hours\n",
    "    invalid_rows = df[(df['dropoff_datetime'] - df['pickup_datetime']) > pd.Timedelta(days=1)] # map\n",
    "    d_long_trip_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    return {\n",
    "        'dropoff_equals_pickup': d_eq_p_res,\n",
    "        'dropoff_before_pickup': d_before_p_res,\n",
    "        'dropoff_year_diff': d_year_diff_res,\n",
    "        'pickup_before_start_date': d_start_date_res,\n",
    "        'pickup_after_end_date': d_pu_end_date_res,\n",
    "        'dropoff_after_end_date': d_do_end_date_res,\n",
    "        'long_trips': d_long_trip_res\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40fec5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_outliers(df, start_date, end_date):\n",
    "    # map: parallel file / partition processing: compares datetime columns independently for each partition in parallel\n",
    "    # reduce: group & aggregate : aggregates results across all partitions\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "\n",
    "    # check if dropoff is equal to pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] == df['pickup_datetime']] # map\n",
    "    d_eq_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff is before pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] < df['pickup_datetime']] # map\n",
    "    d_before_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff year is equal to the pickup year\n",
    "    # tolerate if the difference is 1 day (New Year's Eve to New Year's Day)\n",
    "    invalid_rows = df[(df['dropoff_datetime'].dt.year != df['pickup_datetime'].dt.year) & \n",
    "                      (df['dropoff_datetime'] - df['pickup_datetime'] > pd.Timedelta(days=1))] # map\n",
    "    d_year_diff_res = invalid_rows.groupby('year').size().compute() # reduce \n",
    "\n",
    "    # trip distance should be greater than 0\n",
    "    invalid_rows = df[df['trip_distance'] <= 0] # map\n",
    "    d_trip_distance_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # passenger count should be greater than 0\n",
    "    invalid_rows = df[df['passenger_count'] <= 0] # map\n",
    "    d_passenger_count_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # negative trip distance\n",
    "    invalid_rows = df[df['trip_distance'] < 0] # map\n",
    "    d_trip_distance_neg_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are before start_date\n",
    "    invalid_rows = df[df['pickup_datetime'] < start_date] # map\n",
    "    d_start_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['pickup_datetime'] > end_date] # map\n",
    "    d_pu_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['dropoff_datetime'] > end_date] # map\n",
    "    d_do_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # number of trips longer than 24 hours\n",
    "    invalid_rows = df[(df['dropoff_datetime'] - df['pickup_datetime']) > pd.Timedelta(days=1)] # map\n",
    "    d_long_trip_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    return {\n",
    "        'dropoff_equals_pickup': d_eq_p_res,\n",
    "        'dropoff_before_pickup': d_before_p_res,\n",
    "        'dropoff_year_diff': d_year_diff_res,\n",
    "        'trip_distance_zero': d_trip_distance_res,\n",
    "        'passenger_count_zero': d_passenger_count_res,\n",
    "        'trip_distance_negative': d_trip_distance_neg_res,\n",
    "        'pickup_before_start_date': d_start_date_res,\n",
    "        'pickup_after_end_date': d_pu_end_date_res,\n",
    "        'dropoff_after_end_date': d_do_end_date_res,\n",
    "        'long_trips': d_long_trip_res\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6249335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_outliers_fhv(df, start_date, end_date):\n",
    "    # map: parallel file / partition processing: compares datetime columns independently for each partition in parallel\n",
    "    # reduce: group & aggregate : aggregates results across all partitions\n",
    "    df['year'] = df['pickup_datetime'].dt.year\n",
    "\n",
    "    # check if dropoff is equal to pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] == df['pickup_datetime']] # map\n",
    "    d_eq_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff is before pickup\n",
    "    invalid_rows = df[df['dropoff_datetime'] < df['pickup_datetime']] # map\n",
    "    d_before_p_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check if dropoff year is equal to the pickup year\n",
    "    # tolerate if the difference is 1 day (New Year's Eve to New Year's Day)\n",
    "    invalid_rows = df[(df['dropoff_datetime'].dt.year != df['pickup_datetime'].dt.year) & \n",
    "                      (df['dropoff_datetime'] - df['pickup_datetime'] > pd.Timedelta(days=1))] # map\n",
    "    d_year_diff_res = invalid_rows.groupby('year').size().compute() # reduce \n",
    "\n",
    "    # check how many trips are before start_date\n",
    "    invalid_rows = df[df['pickup_datetime'] < start_date] # map\n",
    "    d_start_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['pickup_datetime'] > end_date] # map\n",
    "    d_pu_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # check how many trips are after end_date\n",
    "    invalid_rows = df[df['dropoff_datetime'] > end_date] # map\n",
    "    d_do_end_date_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    # number of trips longer than 24 hours\n",
    "    invalid_rows = df[(df['dropoff_datetime'] - df['pickup_datetime']) > pd.Timedelta(days=1)] # map\n",
    "    d_long_trip_res = invalid_rows.groupby('year').size().compute() # reduce\n",
    "\n",
    "    return {\n",
    "        'dropoff_equals_pickup': d_eq_p_res,\n",
    "        'dropoff_before_pickup': d_before_p_res,\n",
    "        'dropoff_year_diff': d_year_diff_res,\n",
    "        'pickup_before_start_date': d_start_date_res,\n",
    "        'pickup_after_end_date': d_pu_end_date_res,\n",
    "        'dropoff_after_end_date': d_do_end_date_res,\n",
    "        'long_trips': d_long_trip_res\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07a0189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = {\n",
    "    \"yellow\": pd.Timestamp(\"2012-01-01\"),\n",
    "    \"green\": pd.Timestamp(\"2014-01-01\"),\n",
    "    \"fhv\": pd.Timestamp(\"2015-01-01\"),\n",
    "    \"fhvhv\": pd.Timestamp(\"2019-02-01\"),\n",
    "}\n",
    "end_date = pd.Timestamp(\"2025-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf4069a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_parquet(part_data_path / service_type, engine=\"pyarrow\", assume_missing=True)\n",
    "\n",
    "if service_type == \"fhv\":\n",
    "    result = check_for_outliers_fhv(df, start_date=start_dates[service_type], end_date=end_date)\n",
    "elif service_type == \"fhvhv\":\n",
    "    result = check_for_outliers_fhvhv(df, start_date=start_dates[service_type], end_date=end_date)\n",
    "else:\n",
    "    result = check_for_outliers(df, start_date=start_dates[service_type], end_date=end_date)\n",
    "\n",
    "# for each key in the result, save the DataFrame to a CSV file\n",
    "for key, value in result.items():\n",
    "    value_df = value.reset_index(name='count')\n",
    "    value_df.to_csv(tables_path / f\"{key}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "348e2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the Dask client and cluster\n",
    "client.shutdown()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
