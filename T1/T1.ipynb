{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663ac98c",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T1: Read & split original datasets</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ef8a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2348f9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(\"/d/hpc/projects/FRI/bigdata/data/Taxi\")\n",
    "output_path = Path(\"/d/hpc/projects/FRI/bigdata/students/jv8043/partitioned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ab74c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_lookup = pd.read_csv(\n",
    "    \"/d/hpc/home/jv8043/BD/project/add_data/taxi_zone_lookup.csv\"\n",
    ")\n",
    "# filter only LocationID, latitude, and longitude\n",
    "tz_lookup = tz_lookup[[\"LocationID\", \"latitude\", \"longitude\"]]\n",
    "tz_lookup.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "423f5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(input_path / \"fhv_tripdata_2015-02.parquet\")\n",
    "display(df.head(3))\n",
    "print(df.dtypes)\n",
    "# print unique in SR_Flag\n",
    "print(df[\"SR_Flag\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "759164f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(input_path / \"fhv_tripdata_2024-12.parquet\")\n",
    "display(df.head(3))\n",
    "# print types in df\n",
    "print(df.dtypes)\n",
    "# print unique in SR_Flag\n",
    "print(df[\"SR_Flag\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfde608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_yellow(df):\n",
    "    # payment type dict\n",
    "    payment_type_map = {\n",
    "        \"flex_fare\": 0,\n",
    "        \"credit_card\": 1,\n",
    "        \"cash\": 2,\n",
    "        \"no_charge\": 3,\n",
    "        \"dispute\": 4,\n",
    "        \"unknown\": 5,\n",
    "        \"voided_trip\": 6,\n",
    "    }\n",
    "    \n",
    "    vendor_name_map = {\n",
    "        \"cmt\": 1,\n",
    "        \"curb mobility, llc\": 2,\n",
    "        \"myle technologies inc\": 6,\n",
    "        \"helix\": 7,\n",
    "    }\n",
    "    # Check if 'payment_type' exists and normalize it --> to int\n",
    "    if 'payment_type' in df.columns and df['payment_type'].dtype != 'int64':\n",
    "        df['payment_type'] = df['payment_type'].str.lower().map(payment_type_map).fillna(5).astype(int)\n",
    "    else:\n",
    "        df['payment_type'] = 5\n",
    "\n",
    "    # Normalize 'passenger_count' to int\n",
    "    if 'passenger_count' in df.columns and df['passenger_count'].dtype != 'int64':\n",
    "        df['passenger_count'] = pd.to_numeric(df['passenger_count'], errors='coerce').fillna(1).astype(int)\n",
    "    else:\n",
    "        df['passenger_count'] = 1\n",
    "\n",
    "    # Normalize rate_code Rate_Code to RatecodeID, fill NaNs with 99\n",
    "    if 'rate_code' in df.columns:\n",
    "        df['rate_code'] = pd.to_numeric(df['rate_code'], errors='coerce').fillna(99).astype(int)\n",
    "        df.rename(columns={'rate_code': 'ratecodeid'}, inplace=True)\n",
    "    else:\n",
    "        df['ratecodeid'] = 99\n",
    "\n",
    "    # Normalize store_and_forward to 'store_and_fwd_flag', fill NaNs with 'N'\n",
    "    if 'store_and_forward' in df.columns:\n",
    "        df['store_and_forward'] = df['store_and_forward'].fillna('N')\n",
    "        df['store_and_forward'] = df['store_and_forward'].str.upper().map({'Y': 'Y', 'N': 'N'}).fillna('N')\n",
    "        df.rename(columns={'store_and_forward': 'store_and_fwd_flag'}, inplace=True)\n",
    "    else:\n",
    "        df['store_and_fwd_flag'] = 'N'\n",
    "\n",
    "    # Normalize Fare_Amt to 'fare_amount', fill NaNs with 0.0\n",
    "    if 'fare_amt' in df.columns and 'fare_amount' not in df.columns:\n",
    "        df['fare_amount'] = pd.to_numeric(df['fare_amt'], errors='coerce').fillna(0.0)\n",
    "        df.drop(columns=['fare_amt'], inplace=True)\n",
    "\n",
    "    # Fill values in mta_tax that are NaN with 0.0\n",
    "    if 'mta_tax' in df.columns:\n",
    "        df['mta_tax'] = pd.to_numeric(df['mta_tax'], errors='coerce').fillna(0.0)\n",
    "    else:\n",
    "        df['mta_tax'] = 0.0\n",
    "\n",
    "    # Rename Tip_Amt\tTolls_Amt\tTotal_Amt to 'tip_amount', 'tolls_amount', 'total_amount'\n",
    "    for col in ['tip_amt', 'tolls_amt', 'total_amt']:\n",
    "        if col in df.columns:\n",
    "            new_col = col.replace('_amt', '_amount')\n",
    "            df[new_col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        elif col.replace('_amt', '_amount') not in df.columns:\n",
    "            df[col.replace('_amt', '_amount')] = 0.0\n",
    "\n",
    "    # If there is surcharge column, rename it to 'extra'\n",
    "    if 'surcharge' in df.columns:\n",
    "        df['extra'] = pd.to_numeric(df['surcharge'], errors='coerce').fillna(0.0)\n",
    "        df.drop(columns=['surcharge'], inplace=True)\n",
    "        df['congestion_surcharge'] = 0.0\n",
    "        df['improvement_surcharge'] = 0.0\n",
    "    elif 'extra' not in df.columns:\n",
    "        df['extra'] = 0.0\n",
    "        df['congestion_surcharge'] = 0.0\n",
    "        df['improvement_surcharge'] = 0.0\n",
    "\n",
    "    # If there is no Airport_fee column, set it to 0.0\n",
    "    if 'airport_fee' in df.columns:\n",
    "        df['airport_fee'] = pd.to_numeric(df['airport_fee'], errors='coerce').fillna(0.0)\n",
    "    else:\n",
    "        df['airport_fee'] = 0.0\n",
    "\n",
    "    # Normalize 'vendor_name' to 'vendorid', fill NaNs with 0\n",
    "    if 'vendor_name' in df.columns:\n",
    "        df['vendorid'] = df['vendor_name'].str.lower().map(vendor_name_map).fillna(0).astype(int)\n",
    "        df.drop(columns=['vendor_name'], inplace=True)\n",
    "    elif 'vendorid' not in df.columns:\n",
    "        df['vendorid'] = 0\n",
    "    else: \n",
    "        df['vendorid'] = df['vendorid'].astype(int)\n",
    "\n",
    "    # standardize pickup and dropoff datetime columns    \n",
    "    pickup_cols = ['tpep_pickup_datetime', \"Trip_Pickup_DateTime\", \"pickup_datetime\"]\n",
    "    pickup_col = None\n",
    "    for pickup_col_ in pickup_cols:\n",
    "        if pickup_col_ in df.columns:\n",
    "            pickup_col = pickup_col_\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[pickup_col])\n",
    "    if pickup_col != \"pickup_datetime\":\n",
    "        df.drop(columns=[pickup_col], inplace=True)\n",
    "\n",
    "    dropoff_col = None\n",
    "    for dropoff_col_ in ['trip_dropoff_datetime', 'dropoff_datetime', 'tpep_dropoff_datetime']:\n",
    "        if dropoff_col_ in df.columns:\n",
    "            dropoff_col = dropoff_col_\n",
    "            break\n",
    "    if dropoff_col is None:\n",
    "        print(f\"Dropoff column not found in {df.columns}\")\n",
    "\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df[dropoff_col])\n",
    "    if dropoff_col != \"dropoff_datetime\":\n",
    "        df.drop(columns=[dropoff_col], inplace=True)\n",
    "\n",
    "    # If there are PULocationID and DOLocationID, based on the zone lookup, create pickup and dropoff lat and lon\n",
    "    if 'pulocationid' in df.columns:\n",
    "        df = df.merge(tz_lookup, left_on=\"pulocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"pickup_lat\", \"longitude\": \"pickup_lon\"})\n",
    "        df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "        df = df.merge(tz_lookup, left_on=\"dolocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"dropoff_lat\", \"longitude\": \"dropoff_lon\"})\n",
    "        df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "        # pulocationid and dolocationid to int64\n",
    "        df['pulocationid'] = df['pulocationid'].astype('int64')\n",
    "        df['dolocationid'] = df['dolocationid'].astype('int64')\n",
    "    elif \"start_lon\" in df.columns or \"end_lon\" in df.columns:\n",
    "        df['pickup_lat'] = df['start_lat']\n",
    "        df['pickup_lon'] = df['start_lon']\n",
    "        df.drop(columns=['start_lat', 'start_lon'], inplace=True)\n",
    "        df['dropoff_lat'] = df['end_lat']\n",
    "        df['dropoff_lon'] = df['end_lon']\n",
    "        df.drop(columns=['end_lat', 'end_lon'], inplace=True)\n",
    "    else:\n",
    "        print(\"No pickup or dropoff location columns found in the DataFrame in\", df.columns)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52a96c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_green(df):\n",
    "    standard_schema = {\n",
    "        'vendorid': 'int64',\n",
    "        # 'lpep_pickup_datetime': 'datetime64[us]',\n",
    "        # 'lpep_dropoff_datetime': 'datetime64[us]',\n",
    "        'store_and_fwd_flag': 'string',\n",
    "        'ratecodeid': 'float64',\n",
    "        'pulocationid': 'int64',\n",
    "        'dolocationid': 'int64',\n",
    "        'passenger_count': 'float64',\n",
    "        'trip_distance': 'float64',\n",
    "        'fare_amount': 'float64',\n",
    "        'extra': 'float64',\n",
    "        'mta_tax': 'float64',\n",
    "        'tip_amount': 'float64',\n",
    "        'tolls_amount': 'float64',\n",
    "        'ehail_fee': 'float64',\n",
    "        'improvement_surcharge': 'float64',\n",
    "        'total_amount': 'float64',\n",
    "        'payment_type': 'float64',\n",
    "        'trip_type': 'float64',\n",
    "        'congestion_surcharge': 'float64',\n",
    "    }\n",
    "\n",
    "    # pickup and dropoff datetime columns\n",
    "    pickup_col = \"lpep_pickup_datetime\"\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[pickup_col])\n",
    "    df.drop(columns=[pickup_col], inplace=True)\n",
    "\n",
    "    dropoff_col = \"lpep_dropoff_datetime\"\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df[dropoff_col])\n",
    "    df.drop(columns=[dropoff_col], inplace=True)\n",
    "    \n",
    "    # standardize column types\n",
    "    for col, dtype in standard_schema.items():\n",
    "        if col not in df.columns:\n",
    "            df[col] = pd.NA  # or None, depending on dtype\n",
    "        df[col] = df[col].astype(dtype, errors='ignore')\n",
    "\n",
    "    # get latitude and longitude from tz_lookup\n",
    "    df = df.merge(tz_lookup, left_on=\"pulocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"pickup_lat\", \"longitude\": \"pickup_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    df = df.merge(tz_lookup, left_on=\"dolocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"dropoff_lat\", \"longitude\": \"dropoff_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    # pulocationid and dolocationid to int64\n",
    "    df['pulocationid'] = df['pulocationid'].astype('int64')\n",
    "    df['dolocationid'] = df['dolocationid'].astype('int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6b54836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fhv(df):    \n",
    "    # Replace None with np.nan in 'SR_Flag' and convert to float64\n",
    "    df['sr_flag'] = df['sr_flag'].replace({None: np.nan})\n",
    "    df['sr_flag'] = df['sr_flag'].astype(float)\n",
    "\n",
    "    # Replace NaN in PUlocationID and DOlocationID with 265\n",
    "    df['pulocationid'] = df['pulocationid'].fillna(265)\n",
    "    df['dolocationid'] = df['dolocationid'].fillna(265)\n",
    "    \n",
    "    # get latitude and longitude from tz_lookup\n",
    "    df = df.merge(tz_lookup, left_on=\"pulocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"pickup_lat\", \"longitude\": \"pickup_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    df = df.merge(tz_lookup, left_on=\"dolocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"dropoff_lat\", \"longitude\": \"dropoff_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    # pulocationid and dolocationid to int64\n",
    "    df['pulocationid'] = df['pulocationid'].astype('int64')\n",
    "    df['dolocationid'] = df['dolocationid'].astype('int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08306134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fhvhv(df):\n",
    "    dtype_map = {\n",
    "        'hvfhs_license_num': 'object',\n",
    "        'dispatching_base_num': 'object',\n",
    "        'originating_base_num': 'object',\n",
    "        'request_datetime': 'datetime64[us]',\n",
    "        'on_scene_datetime': 'datetime64[us]',\n",
    "        'pickup_datetime': 'datetime64[us]',\n",
    "        'dropoff_datetime': 'datetime64[us]',\n",
    "        'PULocationID': 'int64',\n",
    "        'DOLocationID': 'int64',\n",
    "        'trip_miles': 'float64',\n",
    "        'trip_time': 'int64',\n",
    "        'base_passenger_fare': 'float64',\n",
    "        'tolls': 'float64',\n",
    "        'bcf': 'float64',\n",
    "        'sales_tax': 'float64',\n",
    "        'congestion_surcharge': 'float64',\n",
    "        'airport_fee': 'float64',\n",
    "        'tips': 'float64',\n",
    "        'driver_pay': 'float64',\n",
    "        'shared_request_flag': 'object',\n",
    "        'shared_match_flag': 'object',\n",
    "        'access_a_ride_flag': 'object',\n",
    "        'wav_request_flag': 'object',\n",
    "        'wav_match_flag': 'object',\n",
    "    }\n",
    "\n",
    "    # Flag columns that need Y/N normalization\n",
    "    flag_columns = [\n",
    "        'shared_request_flag',\n",
    "        'shared_match_flag',\n",
    "        'access_a_ride_flag',\n",
    "        'wav_request_flag',\n",
    "        'wav_match_flag',\n",
    "    ]\n",
    "\n",
    "    for col, dtype in dtype_map.items():\n",
    "        if col in df.columns:\n",
    "            if col == 'airport_fee' and df[col].dtype == 'object':\n",
    "                df[col] = df[col].replace({None: 0.0})\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0).astype('float64')\n",
    "            elif dtype.startswith('datetime64'):\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            elif col in flag_columns:\n",
    "                df[col] = df[col].apply(lambda x: 'Y' if x == 'Y' else 'N').astype('object')\n",
    "            else:\n",
    "                df[col] = df[col].astype(dtype, errors='ignore')\n",
    "\n",
    "\n",
    "    df['pulocationid'] = df['pulocationid'].fillna(265)\n",
    "    df['dolocationid'] = df['dolocationid'].fillna(265)\n",
    "    \n",
    "    # get latitude and longitude from tz_lookup\n",
    "    df = df.merge(tz_lookup, left_on=\"pulocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"pickup_lat\", \"longitude\": \"pickup_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    df = df.merge(tz_lookup, left_on=\"dolocationid\", right_on=\"LocationID\", how=\"left\").rename(columns={\"latitude\": \"dropoff_lat\", \"longitude\": \"dropoff_lon\"})\n",
    "    df.drop(columns=[\"LocationID\"], inplace=True)\n",
    "    # pulocationid and dolocationid to int64\n",
    "    df['pulocationid'] = df['pulocationid'].astype('int64')\n",
    "    df['dolocationid'] = df['dolocationid'].astype('int64')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_service_group(service_name, file_list):\n",
    "    # Process files per taxi service\n",
    "    tables = []\n",
    "    print(f\"Processing service: {service_name}\")\n",
    "\n",
    "    for file_path in file_list:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        schema = df.columns.tolist()\n",
    "\n",
    "        # Normalize column names\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        if service_name == \"yellow\":\n",
    "            df = normalize_yellow(df)\n",
    "        elif service_name == \"green\":\n",
    "            df = normalize_green(df)\n",
    "        elif service_name == \"fhv\":\n",
    "            df = normalize_fhv(df)\n",
    "        elif service_name == \"fhvhv\":\n",
    "            df = normalize_fhvhv(df)\n",
    "            \n",
    "        # common processing steps\n",
    "        df['year'] = df['pickup_datetime'].dt.year\n",
    "        df = df.sort_values(\"pickup_datetime\")\n",
    "        for col in df.select_dtypes(include=['datetime']).columns:\n",
    "            df[col] = df[col].astype('datetime64[us]')\n",
    "\n",
    "        tables.append(pa.Table.from_pandas(df))\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"No data for {service_name}\")\n",
    "        return\n",
    "\n",
    "    combined = pa.concat_tables(tables, promote=True)\n",
    "\n",
    "    service_output_path = output_path / service_name\n",
    "    service_output_path.mkdir(exist_ok=True)\n",
    "    for year in combined['year'].unique():\n",
    "        year_path = service_output_path / str(year)\n",
    "        year_path.mkdir(exist_ok=True)\n",
    "    print(f\"Writing data for {service_name} to {service_output_path}\")\n",
    "\n",
    "    ds.write_dataset(\n",
    "        combined,\n",
    "        base_dir=service_output_path,\n",
    "        format=\"parquet\",\n",
    "        partitioning=[\"year\"],\n",
    "        existing_data_behavior=\"overwrite_or_ignore\",\n",
    "        max_rows_per_file=8_000_000\n",
    "    )\n",
    "\n",
    "    print(f\"Done: {service_name} â†’ {service_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5051872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pickup column options per taxi service\n",
    "pickup_col_map = {\n",
    "    'yellow': ['tpep_pickup_datetime', \"Trip_Pickup_DateTime\", \"pickup_datetime\"],\n",
    "    'green': ['lpep_pickup_datetime'],\n",
    "    'fhv': ['pickup_datetime'],\n",
    "    'fhvhv': ['pickup_datetime']\n",
    "}\n",
    "\n",
    "def detect_service(filename, pickup_col_map):\n",
    "    # Detect service type from filename\n",
    "    for service in pickup_col_map:\n",
    "        if filename.split('_')[0] == service:\n",
    "            return service\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06f40db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group input files by taxi service\n",
    "service_files = {\n",
    "    'yellow': [],\n",
    "    'green': [],\n",
    "    'fhv': [],\n",
    "    'fhvhv': []\n",
    "}\n",
    "\n",
    "for file in input_path.glob(\"*.parquet\"):\n",
    "    service = detect_service(file.name, pickup_col_map)\n",
    "    if service:\n",
    "        service_files[service].append(file)\n",
    "    else:\n",
    "        print(f\"Skipping {file.name}, unknown service type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3168ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for yellow, files from 2012-01 forwards\n",
    "# for green, files from 2014-01 forwards\n",
    "# for fhv, files from 2015-01 forwards\n",
    "# for fhvhv, files from 2019-02 forwards\n",
    "service_files_filtered = {\n",
    "    'yellow': [],\n",
    "    'green': [],\n",
    "    'fhv': [],\n",
    "    'fhvhv': []\n",
    "}\n",
    "for service, files in service_files.items():\n",
    "    if service == 'yellow':\n",
    "        files = [f for f in files if re.search(r'\\d{4}-\\d{2}\\.parquet', f.name) and int(re.search(r'\\d{4}', f.name).group(0)) >= 2012]\n",
    "    elif service == 'green':\n",
    "        files = [f for f in files if re.search(r'\\d{4}-\\d{2}\\.parquet', f.name) and int(re.search(r'\\d{4}', f.name).group(0)) >= 2014]\n",
    "    elif service == 'fhv':\n",
    "        files = [f for f in files if re.search(r'\\d{4}-\\d{2}\\.parquet', f.name) and int(re.search(r'\\d{4}', f.name).group(0)) >= 2015]\n",
    "    elif service == 'fhvhv':\n",
    "        files = [\n",
    "            f for f in files\n",
    "            if re.search(r'\\d{4}-\\d{2}\\.parquet$', f.name)\n",
    "            ((int(re.search(r'\\d{4}', f.name).group(0)) == 2019 and int(re.search(r'\\d{2}', f.name).group(0)) >= 2))\n",
    "        ]\n",
    "\n",
    "    service_files_filtered[service] = files  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6c79830",
   "metadata": {},
   "outputs": [],
   "source": [
    "for service, files in service_files_filtered.items():\n",
    "    print(f\"Found {len(files)} files for service: {service}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7364d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort files in each service by name\n",
    "for service in service_files_filtered:\n",
    "    service_files_filtered[service].sort(key=lambda x: x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91602aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run processing per service\n",
    "service = \"fhv\"\n",
    "files = service_files_filtered[service]\n",
    "process_service_group(service, files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
