{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbacbd1",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T5: External data: events</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236b6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc37d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_path = Path(\"/d/hpc/home/jv8043/BD/project/T5/add_data\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba2c82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_events(start_time, end_time, limit=1000):\n",
    "    \"\"\"\n",
    "    Load NYC events from the JSON API between start_time and end_time.\n",
    "\n",
    "    Parameters:\n",
    "        start_time (str): Start datetime in ISO format (e.g. '2017-01-01T00:00:00')\n",
    "        end_time (str): End datetime in ISO format (e.g. '2017-12-31T23:59:59')\n",
    "        limit (int): Number of records to fetch per page (default is 1000)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing all matching events\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://data.cityofnewyork.us/resource/bkfu-528j.json\"\n",
    "    offset = 0\n",
    "    all_data = []\n",
    "\n",
    "    start_time = start_time.replace(\" \", \"T\")\n",
    "    end_time = end_time.replace(\" \", \"T\")\n",
    "    where_clause = f\"start_date_time between '{start_time}' and '{end_time}'\"\n",
    "\n",
    "    while True:\n",
    "        params = {\n",
    "            \"$limit\": limit,\n",
    "            \"$offset\": offset,\n",
    "            \"$where\": where_clause\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        all_data.extend(data)\n",
    "        offset += limit\n",
    "        print(f\"Loaded {len(all_data)} records so far...\")\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b862b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_time = \"2012-01-01T00:00:00\"\n",
    "last_time = \"2025-02-02T00:00:00\"\n",
    "events_df = load_events(first_time, last_time, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29202d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.to_parquet(\n",
    "    add_data_path / \"nyc_events_full.parquet\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b427ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_  = pd.read_parquet(\n",
    "    add_data_path / \"nyc_events_full.parquet\",\n",
    ")\n",
    "print(f\"Data loaded with {len(df_)} records and {len(df_.columns)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a6dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de2e892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute NaN values in each column\n",
    "nan_counts = df_.isna().sum()\n",
    "print(\"NaN values in each column:\")\n",
    "print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete event_street_side column\n",
    "if 'event_street_side' in events_df.columns:\n",
    "    events_df.drop(columns=['event_street_side'], inplace=True)\n",
    "    print(\"Dropped 'event_street_side' column.\")\n",
    "# drop rows with NaN values\n",
    "events_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5bf7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_location = events_df[\"event_location\"].tolist()\n",
    "\n",
    "# filter only events with event_location\n",
    "events_df = events_df[events_df[\"event_location\"].notnull()]\n",
    "events_df = events_df[events_df[\"event_location\"] != \"\"]\n",
    "events_df = events_df[events_df[\"event_location\"] != \"N/A\"]\n",
    "events_df = events_df[events_df[\"event_location\"] != \"Unknown\"]\n",
    "\n",
    "# filter only events with names\n",
    "events_df = events_df[events_df[\"event_name\"].notnull()]\n",
    "events_df = events_df[events_df[\"event_name\"] != \"\"]\n",
    "events_df = events_df[events_df[\"event_name\"] != \"N/A\"]\n",
    "events_df = events_df[events_df[\"event_name\"] != \"Unknown\"]\n",
    "\n",
    "events_df[\"start_date_time\"] = pd.to_datetime(events_df[\"start_date_time\"], errors='coerce')\n",
    "events_df[\"end_date_time\"] = pd.to_datetime(events_df[\"end_date_time\"], errors='coerce')\n",
    "events_df = events_df[\n",
    "    events_df[\"start_date_time\"].notnull() & \n",
    "    events_df[\"end_date_time\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06101c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df[\"start_date_time\"] = events_df[\"start_date_time\"].astype('datetime64[us]')\n",
    "events_df[\"end_date_time\"] = events_df[\"end_date_time\"].astype('datetime64[us]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "642adde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print unique value sin event_type\n",
    "unique_event_types = events_df[\"event_type\"].unique()\n",
    "print(f\"Unique event types: {len(unique_event_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01a0252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_location(loc):\n",
    "    loc = loc.strip()\n",
    "    if \" between\" in loc:\n",
    "        loc = loc.split(\" between \")[0].strip()        \n",
    "        return f\"{loc}, New York\"\n",
    "    elif \":\" in loc:\n",
    "        parts = [p.strip() for p in loc.split(\":\", 1)]\n",
    "        return f\"{parts[0]}, New York\"\n",
    "    else:\n",
    "        return f\"{loc}, New York\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab5e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df[\n",
    "    [\n",
    "        \"event_name\",\n",
    "        \"start_date_time\",\n",
    "        \"end_date_time\",\n",
    "        \"event_location\",\n",
    "        \"event_type\"\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae1b6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_location_col = events_df[\"event_location\"].tolist()\n",
    "event_location_col = [str(i) for i in event_location_col]\n",
    "event_location_col = [normalize_location(i) for i in event_location_col]\n",
    "\n",
    "unique_locs = list(set(event_location_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4be6ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Filtered events DataFrame has {len(events_df)} records after cleaning.\")\n",
    "events_df.to_parquet(\n",
    "    add_data_path / \"nyc_events_cleaned.parquet\",\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
