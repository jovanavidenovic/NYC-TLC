{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b1c0fe",
   "metadata": {},
   "source": [
    "### Big data course project\n",
    "<strong>T5: External data</strong>\n",
    "\n",
    "Jovana Videnovic & Haris Kupinic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47c5a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import BallTree\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "from datetime import timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_data_path = Path(\"/d/hpc/projects/FRI/bigdata/students/jv8043/partitioned_data\")\n",
    "service_type = \"green\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_path = Path(\"/d/hpc/home/jv8043/BD/project/T5/add_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data_path = Path(\"/d/hpc/projects/FRI/bigdata/students/jv8043/augmented_data_new\") / service_type\n",
    "augmented_data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a0189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dates = {\n",
    "    \"yellow\": pd.Timestamp(\"2012-01-01\"),\n",
    "    \"green\": pd.Timestamp(\"2014-01-01\"),\n",
    "    \"fhv\": pd.Timestamp(\"2015-01-01\"),\n",
    "    \"fhvh\": pd.Timestamp(\"2019-02-01\"),\n",
    "}\n",
    "end_date = pd.Timestamp(\"2025-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf4069a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(part_data_path / service_type, format=\"parquet\", partitioning=\"hive\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07d33ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if service_type in [\"yellow\", \"green\"]:\n",
    "    columns_to_drop = [\n",
    "        \"vendorid\", \"store_and_fwd_flag\", \"extra\", \"mta_tax\", \n",
    "        \"tolls_amount\", \"ehail_fee\", \"improvement_surcharge\", \"congestion_surcharge\"\n",
    "    ]\n",
    "\n",
    "    existing_cols_to_drop = [col for col in columns_to_drop if col in table.schema.names]\n",
    "    table = table.drop(existing_cols_to_drop)\n",
    "\n",
    "    required_cols = [\n",
    "    \"pickup_lon\", \"pickup_lat\", \n",
    "    \"dropoff_lon\", \"dropoff_lat\", \n",
    "    \"pickup_datetime\", \"dropoff_datetime\"\n",
    "    ]\n",
    "\n",
    "    # Apply non-null filters for each required column\n",
    "    for col in required_cols:\n",
    "        table = table.filter(pc.invert(pc.is_null(table[col])))\n",
    "\n",
    "    start = pa.scalar(start_dates[service_type], type=pa.timestamp('us'))\n",
    "    end = pa.scalar(end_date, type=pa.timestamp('us'))\n",
    "\n",
    "    # pickup_datetime within [start, end)\n",
    "    table = table.filter(pc.and_(\n",
    "        pc.greater_equal(table[\"pickup_datetime\"], start),\n",
    "        pc.less(table[\"pickup_datetime\"], end)\n",
    "    ))\n",
    "\n",
    "    # dropoff_datetime within [start, end)\n",
    "    table = table.filter(pc.and_(\n",
    "        pc.greater_equal(table[\"dropoff_datetime\"], start),\n",
    "        pc.less(table[\"dropoff_datetime\"], end)\n",
    "    ))\n",
    "\n",
    "    # pickup_datetime <= dropoff_datetime\n",
    "    table = table.filter(pc.less_equal(table[\"pickup_datetime\"], table[\"dropoff_datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c18c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schools_df = pd.read_csv(add_data_path / \"schools.csv\",)\n",
    "colleges_df = pd.read_csv(add_data_path / \"colleges.csv\")\n",
    "major_ba_df = pd.read_csv(add_data_path / \"major_ba.csv\")\n",
    "weather_df = pd.read_csv(add_data_path / \"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18db2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(schools_df.head(1))\n",
    "display(colleges_df.head(1))\n",
    "display(major_ba_df.head(1))\n",
    "display(weather_df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770b3aa",
   "metadata": {},
   "source": [
    "#### Weather external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdd87d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all columns in weather_df small\n",
    "weather_df.columns = weather_df.columns.str.lower()\n",
    "# count nans in weather_df\n",
    "nan_counts = weather_df.isna().sum()\n",
    "print(\"NaN percentages in weather_df:\")\n",
    "print(nan_counts / weather_df.shape[0]  * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b409e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete cols tavg, name, station\n",
    "weather_df = weather_df.drop(columns=[\"tavg\", \"name\", \"station\"])\n",
    "weather_df[\"awnd\"] = weather_df[\"awnd\"].fillna(0)\n",
    "weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"]) # ns format\n",
    "# convert date to datetime\n",
    "weather_df[\"date\"] = weather_df[\"date\"].astype('datetime64[us]')\n",
    "float_cols = [\n",
    "    \"awnd\",\n",
    "    \"prcp\",\n",
    "    \"snow\",\n",
    "    \"snwd\",\n",
    "    \"tmax\",\n",
    "    \"tmin\",\n",
    "]\n",
    "for col in float_cols:\n",
    "    weather_df[col] = weather_df[col].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c55367e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859d54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df['date'] = weather_df['date'].dt.floor('D')\n",
    "weather_df = weather_df.rename(columns={\"date\": \"pickup_date\"})\n",
    "weather_table = pa.Table.from_pandas(weather_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e234096",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_date = pc.floor_temporal(table[\"pickup_datetime\"], unit=\"day\")\n",
    "table = table.append_column(\"pickup_date\", pickup_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fddf3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table = table.join(\n",
    "    weather_table,\n",
    "    \"pickup_date\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7f435d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "791e1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pickup_date\" in merged_table.schema.names:\n",
    "    merged_table = merged_table.drop([\"pickup_date\"])\n",
    "pickup_year = pc.year(merged_table[\"pickup_datetime\"])\n",
    "merged_table = merged_table.append_column(\"year\", pickup_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ae8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write_dataset(\n",
    "    merged_table,\n",
    "    base_dir=augmented_data_path / \"weather_merged\",\n",
    "    format=\"parquet\",\n",
    "    partitioning=[\"year\"],\n",
    "    existing_data_behavior=\"overwrite_or_ignore\",\n",
    "    max_rows_per_file=6_000_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da550c",
   "metadata": {},
   "source": [
    "#### Schools, colleges, and major bussinesses and atractions external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(\"/d/hpc/projects/FRI/bigdata/students/jv8043/augmented_data/green/weather_merged\", format=\"parquet\", partitioning=\"hive\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f40033",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_scbah_df = pd.read_csv(add_data_path / \"tlc_zones_with_schools_colleges_ba_hotels_strict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "560b455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tlc_scbah_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14b8ee3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_scbah_df = tlc_scbah_df.rename(columns={\"lat\": \"pickup_lat\", \"lon\": \"pickup_lon\"})\n",
    "tlc_scbah_table = pa.Table.from_pandas(tlc_scbah_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38f038a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table1 = table.join(\n",
    "    tlc_scbah_table,\n",
    "    [\"pickup_lat\", \"pickup_lon\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85917deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename this three to closest_school, closest_college, closest_ba with _pickup suffix\n",
    "merged_table1 = merged_table1.rename_columns({\n",
    "    \"closest_school_college\": \"closest_school_college_pickup\",\n",
    "    \"closest_ba\": \"closest_ba_pickup\",\n",
    "    \"closest_hotel\": \"closest_hotel_pickup\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a805803",
   "metadata": {},
   "outputs": [],
   "source": [
    "tlc_scbah_df = pd.read_csv(add_data_path / \"tlc_zones_with_schools_colleges_ba_hotels.csv\")\n",
    "tlc_scbah_df = tlc_scbah_df.rename(columns={\"lat\": \"dropoff_lat\", \"lon\": \"dropoff_lon\"})\n",
    "tlc_scbah_table = pa.Table.from_pandas(tlc_scbah_df, preserve_index=False)\n",
    "merged_table = merged_table1.join(\n",
    "    tlc_scbah_table,\n",
    "    [\"dropoff_lat\", \"dropoff_lon\"],\n",
    ")\n",
    "merged_table = merged_table.rename_columns({\n",
    "    \"closest_school_college\": \"closest_school_college_dropoff\",\n",
    "    \"closest_ba\": \"closest_ba_dropoff\",\n",
    "    \"closest_hotel\": \"closest_hotel_dropoff\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25c561d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d68468",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pickup_date\" in merged_table.schema.names:\n",
    "    merged_table = merged_table.drop([\"pickup_date\"])\n",
    "pickup_year = pc.year(merged_table[\"pickup_datetime\"])\n",
    "merged_table = merged_table.append_column(\"year\", pickup_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfad5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.write_dataset(\n",
    "    merged_table,\n",
    "    base_dir=augmented_data_path / \"weather_scbah\",\n",
    "    format=\"parquet\",\n",
    "    partitioning=[\"year\"],\n",
    "    existing_data_behavior=\"overwrite_or_ignore\",\n",
    "    max_rows_per_file=6_000_000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2a374",
   "metadata": {},
   "source": [
    "### Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85aa491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_parquet(add_data_path / \"nyc_events_augmented.parquet\")\n",
    "events_df[\"start_date_time\"] = pd.to_datetime(events_df[\"start_date_time\"])\n",
    "events_df[\"end_date_time\"] = pd.to_datetime(events_df[\"end_date_time\"])\n",
    "events_df[\"start_date_time\"] = events_df[\"start_date_time\"].astype('datetime64[us]')\n",
    "events_df[\"end_date_time\"] = events_df[\"end_date_time\"].astype('datetime64[us]')\n",
    "display(events_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d70ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_table = pa.Table.from_pandas(events_df, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.dataset(\"/d/hpc/projects/FRI/bigdata/students/jv8043/augmented_data_new/green/weather_scbah\", format=\"parquet\", partitioning=\"hive\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d547e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add row indices to both tables for later reconstruction\n",
    "taxi_with_idx = table.add_column(0, \"taxi_idx\", pa.array(range(len(table))))\n",
    "events_with_idx = events_table.add_column(0, \"event_idx\", pa.array(range(len(events_table))))\n",
    "\n",
    "# Create time windows for events\n",
    "# Convert to microseconds for timestamp arithmetic\n",
    "hour_in_microseconds = 3600 * 1000000\n",
    "\n",
    "# Create expanded time windows\n",
    "events_expanded = events_with_idx.add_column(\n",
    "    len(events_with_idx.column_names),\n",
    "    \"pickup_window_start\", \n",
    "    pc.subtract(events_with_idx[\"start_date_time\"], pa.scalar(hour_in_microseconds, pa.int64()))\n",
    ")\n",
    "\n",
    "events_expanded = events_expanded.add_column(\n",
    "    len(events_expanded.column_names),\n",
    "    \"pickup_window_end\",\n",
    "    events_expanded[\"end_date_time\"]\n",
    ")\n",
    "\n",
    "events_expanded = events_expanded.add_column(\n",
    "    len(events_expanded.column_names),\n",
    "    \"dropoff_window_start\",\n",
    "    events_expanded[\"start_date_time\"]\n",
    ")\n",
    "\n",
    "events_expanded = events_expanded.add_column(\n",
    "    len(events_expanded.column_names),\n",
    "    \"dropoff_window_end\",\n",
    "    pc.add(events_expanded[\"end_date_time\"], pa.scalar(hour_in_microseconds, pa.int64()))\n",
    ")\n",
    "\n",
    "# Function to perform conditional join with time windows\n",
    "def conditional_join_with_time_window(taxi_table, events_table, \n",
    "                                    taxi_time_col, taxi_lat_col, taxi_lon_col,\n",
    "                                    window_start_col, window_end_col):\n",
    "    \"\"\"\n",
    "    Perform a conditional join based on time windows and exact location match\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add dummy key for cross join\n",
    "    taxi_cross = taxi_table.add_column(len(taxi_table.column_names), \"key\", pa.array([1] * len(taxi_table)))\n",
    "    events_cross = events_table.add_column(len(events_table.column_names), \"key\", pa.array([1] * len(events_table)))\n",
    "    \n",
    "    # Perform cross join\n",
    "    joined = taxi_cross.join(events_cross, keys=[\"key\"], join_type=\"inner\")\n",
    "    \n",
    "    # Create filter conditions\n",
    "    # Time condition: taxi_time >= window_start AND taxi_time <= window_end\n",
    "    time_condition = pc.and_(\n",
    "        pc.greater_equal(joined[taxi_time_col], joined[window_start_col]),\n",
    "        pc.less_equal(joined[taxi_time_col], joined[window_end_col])\n",
    "    )\n",
    "    \n",
    "    # Location condition: exact match on lat/lon\n",
    "    location_condition = pc.and_(\n",
    "        pc.equal(joined[taxi_lat_col], joined[\"latitude\"]),\n",
    "        pc.equal(joined[taxi_lon_col], joined[\"longitude\"])\n",
    "    )\n",
    "    \n",
    "    # Combined condition\n",
    "    combined_condition = pc.and_(time_condition, location_condition)\n",
    "    \n",
    "    # Filter the joined table\n",
    "    filtered = joined.filter(combined_condition)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# Process pickup events\n",
    "print(\"Processing pickup events...\")\n",
    "pickup_matches = conditional_join_with_time_window(\n",
    "    taxi_with_idx, events_expanded,\n",
    "    \"pickup_datetime\", \"pickup_lat\", \"pickup_lon\",\n",
    "    \"pickup_window_start\", \"pickup_window_end\"\n",
    ")\n",
    "\n",
    "# Process dropoff events  \n",
    "print(\"Processing dropoff events...\")\n",
    "dropoff_matches = conditional_join_with_time_window(\n",
    "    taxi_with_idx, events_expanded,\n",
    "    \"dropoff_datetime\", \"dropoff_lat\", \"dropoff_lon\", \n",
    "    \"dropoff_window_start\", \"dropoff_window_end\"\n",
    ")\n",
    "\n",
    "# Handle multiple matches - take first match for each taxi ride\n",
    "def get_first_match_per_taxi(matches_table):\n",
    "    \"\"\"\n",
    "    Group by taxi_idx and take the first event_type for each taxi ride\n",
    "    \"\"\"\n",
    "    if len(matches_table) == 0:\n",
    "        return pa.table({\n",
    "            \"taxi_idx\": pa.array([], type=pa.int64()),\n",
    "            \"event_type\": pa.array([], type=pa.string())\n",
    "        })\n",
    "    \n",
    "    # Sort by taxi_idx, then by event_idx to ensure consistent ordering\n",
    "    sorted_matches = matches_table.sort_by([\n",
    "        (\"taxi_idx\", \"ascending\"),\n",
    "        (\"event_idx\", \"ascending\")\n",
    "    ])\n",
    "    \n",
    "    # Group by taxi_idx and take first of each group\n",
    "    # Since PyArrow doesn't have a direct group_by.first(), we'll use a different approach\n",
    "    taxi_indices = sorted_matches[\"taxi_idx\"].to_pylist()\n",
    "    event_types = sorted_matches[\"event_type\"].to_pylist()\n",
    "    \n",
    "    # Find first occurrence of each taxi_idx\n",
    "    seen_taxis = set()\n",
    "    first_matches_taxi = []\n",
    "    first_matches_event = []\n",
    "    \n",
    "    for taxi_idx, event_type in zip(taxi_indices, event_types):\n",
    "        if taxi_idx not in seen_taxis:\n",
    "            seen_taxis.add(taxi_idx)\n",
    "            first_matches_taxi.append(taxi_idx)\n",
    "            first_matches_event.append(event_type)\n",
    "    \n",
    "    return pa.table({\n",
    "        \"taxi_idx\": pa.array(first_matches_taxi),\n",
    "        \"event_type\": pa.array(first_matches_event)\n",
    "    })\n",
    "\n",
    "# Get first matches for pickup and dropoff\n",
    "pickup_first = get_first_match_per_taxi(pickup_matches)\n",
    "dropoff_first = get_first_match_per_taxi(dropoff_matches)\n",
    "\n",
    "# Create lookup arrays for the original table\n",
    "pickup_lookup = pa.nulls(len(table), type=pa.string())\n",
    "dropoff_lookup = pa.nulls(len(table), type=pa.string())\n",
    "\n",
    "# Fill in the matched events\n",
    "if len(pickup_first) > 0:\n",
    "    pickup_indices = pickup_first[\"taxi_idx\"].to_pylist()\n",
    "    pickup_events = pickup_first[\"event_type\"].to_pylist()\n",
    "    \n",
    "    # Create a new array with the matched events\n",
    "    pickup_array = pa.nulls(len(table), type=pa.string()).to_pylist()\n",
    "    for idx, event in zip(pickup_indices, pickup_events):\n",
    "        pickup_array[idx] = event\n",
    "    pickup_lookup = pa.array(pickup_array)\n",
    "\n",
    "if len(dropoff_first) > 0:\n",
    "    dropoff_indices = dropoff_first[\"taxi_idx\"].to_pylist()\n",
    "    dropoff_events = dropoff_first[\"event_type\"].to_pylist()\n",
    "    \n",
    "    # Create a new array with the matched events  \n",
    "    dropoff_array = pa.nulls(len(table), type=pa.string()).to_pylist()\n",
    "    for idx, event in zip(dropoff_indices, dropoff_events):\n",
    "        dropoff_array[idx] = event\n",
    "    dropoff_lookup = pa.array(dropoff_array)\n",
    "\n",
    "# Add the new columns to the original table\n",
    "final_table = table.add_column(len(table.column_names), \"pickup_event_type\", pickup_lookup)\n",
    "final_table = final_table.add_column(len(final_table.column_names), \"dropoff_event_type\", dropoff_lookup)\n",
    "\n",
    "print(f\"Total taxi rides: {len(final_table)}\")\n",
    "print(f\"Rides with pickup events: {pc.sum(pc.is_valid(final_table['pickup_event_type'])).as_py()}\")\n",
    "print(f\"Rides with dropoff events: {pc.sum(pc.is_valid(final_table['dropoff_event_type'])).as_py()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
